{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Deterministic Policy Gradients (DDPG)\n",
    "---\n",
    "In this notebook, we train DDPG with OpenAI Gym's BipedalWalker-v2 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render ai gym environment\n",
    "# import gym\n",
    "import gymnasium as gym  # new version of gym\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from ddpg_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gymnasium.farama.org/environments/box2d/bipedal_walker/\n",
    "# env = gym.make('BipedalWalker-v2')\n",
    "env = gym.make('BipedalWalker-v3', render_mode=\"rgb_array\")\n",
    "# env.seed(10)\n",
    "agent = Agent(state_size=env.observation_space.shape[0], action_size=env.action_space.shape[0], random_seed=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load checkpoint if desired.\n",
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DDPG\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  Alternatively, you can skip to the next code cell to load the pre-trained weights from file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -37.68\t Memory Size 66478ory Size 66478                  \n",
      "Episode 200\tAverage Score: -56.31\t Memory Size 108468y Size 108468                    \n",
      "Episode 300\tAverage Score: -57.00\t Memory Size 116864ory Size 116864                  \n",
      "Episode 400\tAverage Score: -21.83\t Memory Size 134118ry Size 134118                   \n",
      "Episode 500\tAverage Score: -11.53\t Memory Size 151002y Size 151002                   \n",
      "Episode 600\tAverage Score: -34.70\t Memory Size 185723 Size 185723                     \n",
      "Episode 700\tAverage Score: -17.17\t Memory Size 198774y Size 198774                    \n",
      "Episode 800\tAverage Score: -26.63\t Memory Size 200000ry Size 200000                   \n",
      "Episode 900\tAverage Score: -57.27\t Memory Size 200000ry Size 200000                   \n",
      "Episode 1000\tAverage Score: -77.28\t Memory Size 200000ory Size 200000                  \n",
      "Episode 1100\tAverage Score: -25.54\t Memory Size 200000y Size 200000                    \n",
      "Episode 1200\tAverage Score: -52.80\t Memory Size 200000ry Size 200000                   \n",
      "Episode 1300\tAverage Score: -62.79\t Memory Size 200000ory Size 200000                  \n",
      "Episode 1400\tAverage Score: -102.29\t Memory Size 200000ory Size 200000                  \n",
      "Episode 1500\tAverage Score: -82.32\t Memory Size 200000y Size 200000                     \n",
      "Episode 1600\tAverage Score: -31.37\t Memory Size 200000y Size 200000                    \n",
      "Episode 1645\tAverage Score: -12.39\tScore: -51.96\t Memory Size 200000                   "
     ]
    }
   ],
   "source": [
    "def ddpg(n_episodes=2000, max_t=700, print_every=100):\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    max_score = -np.Inf\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # state = env.reset()\n",
    "        state, _ = env.reset()                                    # new gymnasium\n",
    "        agent.reset()\n",
    "        score = 0\n",
    "        action = np.array([0., 0., 0., 0.])\n",
    "        for t in range(max_t):\n",
    "            prev_action = action\n",
    "            action = agent.act(state)\n",
    "            # next_state, reward, done, _ = env.step(action)\n",
    "            next_state, reward, done, _, _ = env.step(action)     # new gymnasium\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            # the agent is stuck in a \"not done\" state, but its actions \n",
    "            # are not effecting any change on the environment state\n",
    "            # probably shouldn't do an equality... probably should have some tolerance\n",
    "            if np.max(np.abs(prev_action - action)) < 0.0001 and np.max(np.abs(state - next_state)) < 0.0001:\n",
    "                # print(\"stuck\")\n",
    "                break\n",
    "            \n",
    "        scores_deque.append(score)\n",
    "        scores.append(score)\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}\\tScore: {:.2f}\\t\\t Memory Size {}                  '.format(i_episode, np.mean(scores_deque), score, len(agent.memory.memory)), end=\"\")\n",
    "        if i_episode % print_every == 0:\n",
    "            torch.save(agent.actor_local.state_dict(), 'checkpoint_actor.pth')\n",
    "            torch.save(agent.critic_local.state_dict(), 'checkpoint_critic.pth')\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}\\t\\t Memory Size {}                 '.format(i_episode, np.mean(scores_deque), len(agent.memory.memory)))   \n",
    "    return scores\n",
    "\n",
    "scores = ddpg()\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episode 100\tAverage Score: -100.43\tScore: -99.22\n",
    "Episode 200\tAverage Score: -100.50\tScore: -97.223\n",
    "Episode 300\tAverage Score: -90.31\tScore: -97.313\n",
    "Episode 400\tAverage Score: -94.41\tScore: -97.18\n",
    "Episode 500\tAverage Score: -93.65\tScore: -96.80\n",
    "Episode 600\tAverage Score: -94.38\tScore: -96.97\n",
    "Episode 700\tAverage Score: -97.19\tScore: -97.86\n",
    "Episode 800\tAverage Score: -93.05\tScore: -97.32\n",
    "Episode 900\tAverage Score: -98.05\tScore: -101.53\n",
    "Episode 1000\tAverage Score: -95.61\tScore: -100.68\n",
    "Episode 1100\tAverage Score: -76.18\tScore: -97.060\n",
    "Episode 1200\tAverage Score: -60.73\tScore: -37.78\n",
    "Episode 1300\tAverage Score: -39.58\tScore: -41.27\n",
    "Episode 1314\tAverage Score: -40.41\tScore: -31.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should already be installed in my docker file\n",
    "# !python -m pip install pyvirtualdisplay\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()\n",
    "\n",
    "is_ipython = 'inline' in plt.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.actor_local.load_state_dict(torch.load('checkpoint_actor.pth'))\n",
    "agent.critic_local.load_state_dict(torch.load('checkpoint_critic.pth'))\n",
    "\n",
    "# state = env.reset()\n",
    "state, _ = env.reset()\n",
    "\n",
    "# img = plt.imshow(env.render(mode='rgb_array'))\n",
    "img = plt.imshow(env.render())\n",
    "\n",
    "agent.reset()   \n",
    "while True:\n",
    "    action = agent.act(state)\n",
    "    #\n",
    "    # save to output\n",
    "    #\n",
    "    # env.render()\n",
    "    img.set_data(env.render()) \n",
    "    plt.axis('off')\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    state = next_state\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore\n",
    "\n",
    "In this exercise, we have provided a sample DDPG agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster than this benchmark implementation.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task!\n",
    "- Write your own DDPG implementation.  Use this code as reference only when needed -- try as much as you can to write your own algorithm from scratch.\n",
    "- You may also like to implement prioritized experience replay, to see if it speeds learning.  \n",
    "- The current implementation adds Ornsetein-Uhlenbeck noise to the action space.  However, it has [been shown](https://blog.openai.com/better-exploration-with-parameter-noise/) that adding noise to the parameters of the neural network policy can improve performance.  Make this change to the code, to verify it for yourself!\n",
    "- Write a blog post explaining the intuition behind the DDPG algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
